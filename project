#22/10/18
#rename files?

import os

path = 'C:\\Users\\Amy\\Desktop\\Com. Science\\Project\\SeamusVennDiagram'
files = os.listdir(path)
i = 1

for file in files:
    filename, file_extension = os.path.splitext(file)
    os.rename(os.path.join(path, file), os.path.join(path, filename + str(i) + file_extension))
    i = i+1


#cropped images - all diff sizes?

cropped = np.empty(len(onlyfiles), dtype=object)

for n in range(0, len(onlyfiles)):
	  cropped[n] = images[n][50:730, 350:1520]

print('img1 type:',type(cropped[16]),'and shape:', np.shape(cropped[16]))
#cv2.imshow('image[43]', cropped[43])


#gray vs colour sub

images = np.empty(len(onlyfiles), dtype=object)
gray = np.empty(len(onlyfiles), dtype=object)

for n in range(0, len(onlyfiles)):
  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )
	  gray[n] = cv2.cvtColor(resizeImages[n], cv2.COLOR_BGR2GRAY)

subsag = np.empty(len(onlyfiles), dtype=object)
subsa = np.empty(len(onlyfiles), dtype=object)

for i in range(1, len(onlyfiles)-1):
  subsag[i] = cv2.subtract(gray[i], gray[len(onlyfiles)-1])
  subsa[i] = cv2.subtract(resizeImages[i], resizeImages[len(onlyfiles)-1])
	
cv2.imshow('cv2_subsag30', subsag[30])
cv2.imshow('cv2_subsa30', subsa[30])


#sol-ans vs ans-sol

subsa = np.empty(len(onlyfiles), dtype=object)
subas = np.empty(len(onlyfiles), dtype=object)

for i in range(1, len(onlyfiles)-1):
  subsa[i] = cv2.subtract(gray[i], gray[len(onlyfiles)-1])
  subas[i] = cv2.subtract(gray[len(onlyfiles)-1], gray[i])



#23/10/18

#contours of an image onto another image
#could make array to put contours on them all?

img = gray[44]
img1 = resizeImages[26]
ret,thresh = cv2.threshold(img,127,255,0)
im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
cv2.imshow('contours',cv2.drawContours(img1, contours, -1, (0,255,0), 1))


#find a template in an image 
img = cv2.imread('C:\\Users\\Amy\\Desktop\\Com. Science\\Project\\SeamusVennDiagram\\student26.jpg',0)
img2 = img.copy()
template = cv2.imread('C:\\Users\\Amy\\Desktop\\Com. Science\\Project\\SeamusVennDiagram\\student44.jpg',0)
w, h = template.shape[::-1]
 
# All the 6 methods for comparison in a list
methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR',
           'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']

for meth in methods:
  img = img2.copy()
  method = eval(meth)
 
  # Apply template Matching  
  res = cv2.matchTemplate(img,template,method)
  min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)
 
  # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum
  if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
    top_left = min_loc
  else:
    top_left = max_loc
    bottom_right = (top_left[0] + w, top_left[1] + h)
 
  cv2.rectangle(img,top_left, bottom_right, 255, 2)
 
  plt.subplot(121),plt.imshow(res,cmap = 'gray')
  plt.title('Matching Result'), plt.xticks([]), plt.yticks([])
  plt.subplot(122),plt.imshow(img,cmap = 'gray')
  plt.title('Detected Point'), plt.xticks([]), plt.yticks([])
  plt.suptitle(meth)

  plt.show()
  
  
  
  #27/10/18
  #count no. of coloured pixels - not working
  boundaries = [
	([17, 15, 100], [50, 56, 200]),
	([86, 31, 4], [220, 88, 50]),
	([25, 146, 190], [62, 174, 250]),
	([103, 86, 65], [145, 133, 128])
]

# loop over the boundaries
for (lower, upper) in boundaries:
	# create NumPy arrays from the boundaries
  lower = np.array(lower, dtype = "uint8")
  upper = np.array(upper, dtype = "uint8")
 
	# find the colors within the specified boundaries and apply
	# the mask
  mask = cv2.inRange(img, lower, upper)
  output = cv2.bitwise_and(img, img, mask = mask)
  cv2.imshow("images", np.hstack([img, output]))
  
  
  
  #28/10/18
  #counting num of changed pixels Painfully slow!
  #base black taken from student 24 due to lecturer being a diff size
# base black is 330 pixels

pix = []
pixans = []

for n in range(0, len(gray)):
  countg = 0
  for i in range(0, 450):
    for j in range(0, 500):
      if np.all(gray[n][i,j] != [255,255,255]):
        countg += 1
  pix.append(countg)

numPix = 0

for n in range(0, len(pix)):
  numPix = pix[44]-pix[n]
  pixans.append(numPix)
  print(str(n), pixans[n])
  numPix = 0

print(pix[44])


#score and diff
#https://www.pyimagesearch.com/2017/06/19/image-difference-with-opencv-and-python/

i=0
j=0
right = np.empty(len(onlyfiles), dtype=object)
wrong = np.empty(len(onlyfiles), dtype=object)
for n in range(0, len(onlyfiles)):
	(score, diff) = compare_ssim(gray[42], gray[n], full=True)
	diff = (diff * 255).astype("uint8")
	#if score >=0.9123:
	print(str(n), "SSIM: {}".format(score))
		#right[i]=gray[n]
		#i+=1
	#else:
	#	wrong[j]=gray[n]
	#	j+=1
	score = 0
	diff = 0
	
#contours - not working
thresh = cv2.threshold(diff, 0, 255,
	cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,
	cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if imutils.is_cv2() else cnts[1]

for c in cnts:
	# compute the bounding box of the contour and then draw the
	# bounding box on both input images to represent where the two
	# images differ
	(x, y, w, h) = cv2.boundingRect(c)
	cv2.rectangle(gray[44], (x, y), (x + w, y + h), (0, 0, 255), 2)
	cv2.rectangle(gray[25], (x, y), (x + w, y + h), (0, 0, 255), 2)
 
# show the output images
cv2.imshow("Original", gray[44])
cv2.imshow("Modified", gray[25])
cv2.imshow("Diff", diff)
cv2.imshow("Thresh", thresh)

  
3/11/18
#sql
import mysql.connector
from mysql.connector import errorcode

try:
  cnx = mysql.connector.connect(user='root',
                                database='UniDoodle',
				password= '')
																password= '')
except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print("Something is wrong with your user name or password")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print("Database does not exist")
  else:
    print(err)
else:
  cnx.close()
  
  
#5/11/18
#SIFT - sift has problems in opencv 3.4
def drawMatches(img1, kp1, img2, kp2, matches):

    rows1 = img1.shape[0]
    cols1 = img1.shape[1]
    rows2 = img2.shape[0]
    cols2 = img2.shape[1]

    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')
    out[:rows1,:cols1] = np.dstack([img1])
    out[:rows2,cols1:] = np.dstack([img2])
    for mat in matches:
        img1_idx = mat.queryIdx
        img2_idx = mat.trainIdx
        (x1,y1) = kp1[img1_idx].pt
        (x2,y2) = kp2[img2_idx].pt

        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0, 1), 1)   
        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0, 1), 1)
        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (255, 0, 0, 1), 1)

    return out

def compare(filename1, filename2):
    img1 = cv2.imread(filename1)          # queryImage
    img2 = cv2.imread(filename2)          # trainImage

    # Initiate SIFT detector
    sift = cv2.SIFT()

    # find the keypoints and descriptors with SIFT
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)

    # BFMatcher with default params
    bf = cv2.BFMatcher()
    matches = bf.match(des1,des2)

    matches = sorted(matches, key=lambda val: val.distance)

    img3 = drawMatches(img1,kp1,img2,kp2,matches[:25])

    # Show the image
    cv2.imshow('Matched Features', img3)
    cv2.waitKey(0)
    cv2.destroyWindow('Matched Features')

if len(sys.argv) != 3:
    sys.stderr.write("usage: compare.py <queryImageFile> <sourceImageFile>\n")
    sys.exit(-1)
    
compare(sys.argv[1], sys.argv[2])


#contours from correct ans onto other images
for n in range(0, len(resizeImages)):
	img = images[44]
	img1 = images[n]
	ret,thresh = cv2.threshold(img,127,255,0)
	im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
	cont[n]=cv2.drawContours(img1, contours, -1, (0,128,0), 1)

cv2.imshow('image 5', cont[5])
cv2.imshow('image 25', cont[25])
cv2.imshow('image 35', cont[35])
cv2.imshow('image 40', cont[40])


#SSIM score and diff
i=0
j=0

for n in range(0, len(onlyfiles)):
	(score, diff) = compare_ssim(gray[42], gray[n], full=True)
	diff = (diff * 255).astype("uint8")
	#if score >=0.9123:
	print(str(n), "SSIM: {}".format(score))
		#right[i]=images[n]
		#i+=1
	#else:
	#	wrong[j]=images[n]
	#	j+=1
	score = 0
	diff = 0

print(i,j)


##8/11/18
##image recognision

from __future__ import print_function
import cv2
import numpy as np
 
 
MAX_FEATURES = 500
GOOD_MATCH_PERCENT = 0.15
 
 
def alignImages(im1, im2):
 
  # Convert images to grayscale
  im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
  im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)
   
  # Detect ORB features and compute descriptors.
  orb = cv2.ORB_create(MAX_FEATURES)
  keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)
  keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)
   
  # Match features.
  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
  matches = matcher.match(descriptors1, descriptors2, None)
   
  # Sort matches by score
  matches.sort(key=lambda x: x.distance, reverse=False)
 
  # Remove not so good matches
  numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)
  matches = matches[:numGoodMatches]
 
  # Draw top matches
  imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)
  cv2.imwrite("matches.jpg", imMatches)
   
  # Extract location of good matches
  points1 = np.zeros((len(matches), 2), dtype=np.float32)
  points2 = np.zeros((len(matches), 2), dtype=np.float32)
 
  for i, match in enumerate(matches):
    points1[i, :] = keypoints1[match.queryIdx].pt
    points2[i, :] = keypoints2[match.trainIdx].pt
   
  # Find homography
  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)
 
  # Use homography
  height, width, channels = im2.shape
  im1Reg = cv2.warpPerspective(im1, h, (width, height))
   
  return im1Reg, h
 
 
if __name__ == '__main__':
   
  # Read reference image
  refFilename = "C:\\Users\\Amy\\Desktop\\Com. Science\\Project\\SeamusVennDiagram\\student42.jpg"
  print("Reading reference image : ", refFilename)
  imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)
 
  # Read image to be aligned
  imFilename = "C:\\Users\\Amy\\Desktop\\Com. Science\\Project\\SeamusVennDiagram\\student25.jpg"
  print("Reading image to align : ", imFilename);  
  im = cv2.imread(imFilename, cv2.IMREAD_COLOR)
   
  print("Aligning images ...")
  # Registered image will be resotred in imReg. 
  # The estimated homography will be stored in h. 
  imReg, h = alignImages(im, imReference)
   
  # Write aligned image to disk. 
  outFilename = "aligned.jpg"
  print("Saving aligned image : ", outFilename); 
  cv2.imwrite(outFilename, imReg)
 
  # Print estimated homography
  print("Estimated homography : \n",  h)
  
  
  #13/11/18
  #hist and template matching
  #method 1
def get_image_difference(image_1, image_2):
    first_image_hist = cv2.calcHist([image_1], [0], None, [256], [0, 256])
    second_image_hist = cv2.calcHist([image_2], [0], None, [256], [0, 256])
    img_hist_diff = cv2.compareHist(first_image_hist, second_image_hist, cv2.HISTCMP_BHATTACHARYYA)
    img_template_probability_match = cv2.matchTemplate(first_image_hist, second_image_hist, cv2.TM_CCOEFF_NORMED)[0][0]
    img_template_diff = 1 - img_template_probability_match

    # taking only 10% of histogram diff, since it's less accurate than template method
    commutative_image_diff = (img_hist_diff / 10) + img_template_diff
    return commutative_image_diff

minimum_commutative_image_diff = 1
image_1 =gray[31]
image_2 = gray[42]
commutative_image_diff = get_image_difference(image_1, image_2)

if commutative_image_diff < minimum_commutative_image_diff:      
    print("Matched")
    print(commutative_image_diff) 
else:
    print("fail") #random failure value

#method 2
src_base = resize[44]
hsv_base = cv2.cvtColor(src_base, cv2.COLOR_BGR2HSV)

hsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]
h_bins = 50
s_bins = 60
histSize = [h_bins, s_bins]
# hue varies from 0 to 179, saturation from 0 to 255
h_ranges = [0, 180]
s_ranges = [0, 256]
ranges = h_ranges + s_ranges # concat lists
# Use the 0-th and 1-st channels
channels = [0, 1]


hist_base = cv2.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)
cv2.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)

for n in range (0, len(onlyfiles)):
    hsv_test1 = cv2.cvtColor(resize[n], cv2.COLOR_BGR2HSV)
    hist_test1 = cv2.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)
    cv2.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)

    for compare_method in range(4):
        base_test1 = cv2.compareHist(hist_base, hist_test1, compare_method)
            print(str(n), 'Method:', compare_method, ' Base-Test(1) :', base_test1)
#1=correlation, 2=chi-square, 3=intersection, 4=bhattacharyya

#crop, resze and SSIM
def crop_image(img):
    #prevent changes to original image
    img_o = img.copy()
    
    gray = cv2.cvtColor(img_o, cv2.COLOR_BGR2GRAY) # convert to grayscale

    # threshold 
    retval, thresh_gray = cv2.threshold(gray, thresh=200, maxval=256, type=cv2.THRESH_BINARY)

    #find black pixels
    points = np.argwhere(thresh_gray==0) 
    #store in x, y coords
    points = np.fliplr(points)     
    
    x, y, w, h = cv2.boundingRect(points) 
    
    #expand box and do not allow negative (image may be x,y 0,0)
    x, y, w, h = x-10 if x-10>0 else 0, y-10 if y-10>0 else 0,w+20, h+20 
    #print(x,y,w,h)
    
    # create a cropped region of the gray image
    crop = img[y:y+h, x:x+w] 

    return crop

#orig_img = crop_image(orig_img)
#comp_img = crop_image(comp_img)

size = []

for n in range(0, len(onlyfiles)):
    crop[n] = crop_image(image[n])
    size.append(crop[n].shape)

#identify smallest image size
small = min(size)[:2][::-1]

for n in range(0, len(onlyfiles)):
    resize[n] = cv2.resize(crop[n], dsize = small)
    gray[n] = cv2.cvtColor(resize[n].copy(), cv2.COLOR_BGR2GRAY)


for n in range(0, len(onlyfiles)):
    (score, diff) = compare_ssim(resize[44], resize[n], full=True, multichannel=True)
    print(str(n),"SSIM: %.2f" % (score))
    (score, diff) = compare_ssim(gray[44], gray[n], full=True,multichannel=False,gaussian_weights=True)
    print(str(n),"Gray SSIM: %.2f" % (score))

